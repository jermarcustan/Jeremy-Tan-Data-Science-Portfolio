{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c369041-f76b-42d1-a4df-ef8738827e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7   \n",
       "0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815  \\\n",
       "1  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
       "\n",
       "         x8        x9       x10  ...       x21       x22       x23       x24   \n",
       "0  0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793  0.391253  \\\n",
       "1  0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938  0.336440   \n",
       "2  0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939  0.289354   \n",
       "3  0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607  0.223826   \n",
       "4  0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392  0.401270   \n",
       "\n",
       "        x25       x26       x27       x28       x29  y  \n",
       "0  0.585122  0.394557  0.418976  0.312697  0.005824  1  \n",
       "1  0.587290  0.446013  0.416345  0.313423  0.000105  1  \n",
       "2  0.559515  0.402727  0.415489  0.311911  0.014739  1  \n",
       "3  0.614245  0.389197  0.417669  0.314371  0.004807  1  \n",
       "4  0.566343  0.507497  0.420561  0.317490  0.002724  1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"creditcardfraud.csv\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Number of features\n",
    "num_features = df.shape[1]\n",
    "\n",
    "# Create column names: x1, x2, ..., x29\n",
    "column_names = [f\"x{i}\" for i in range(1, num_features )]\n",
    "\n",
    "# Add the target column name 'y'\n",
    "column_names.append(\"y\")\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bebe824-8949-493e-837d-6a5d12430848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1    284315\n",
       "-1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].value_counts()\n",
    "\n",
    "# 1 are the normal, -1 are the anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ec4fda-b1a0-4050-a667-517f1294005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1\n",
    "df_normal = df[df[\"y\"] == 1]\n",
    "\n",
    "# y = -1\n",
    "df_anomaly = df[df[\"y\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b925a8-16be-4cd7-8a4d-5f411357fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set \n",
    "# 492 1's and 492 -1's\n",
    "\n",
    "df_test_ones = df_normal.sample(492)\n",
    "df_test = pd.concat([df_test_ones, df_anomaly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f6304f-7dbf-4306-b388-cc5a859e3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = df_test.index.tolist()\n",
    "df_train = df.drop(indices_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f56a406-701d-46a1-8d2f-3c416e8dc79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "1    283823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1617fb0-68c9-4af5-929f-efb814123eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1    284315\n",
       "-1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fff9f0-da05-4f25-b779-c326d680c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = df_train.drop(\"y\", axis = 1)\n",
    "df_train_y = df_train[\"y\"]\n",
    "df_test_x = df_test.drop(\"y\", axis = 1)\n",
    "df_test_y = df_test[\"y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eac1ed6e-0acd-44d3-a737-5fc5c4c6a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f14846-1aa6-45a1-b35c-2d2ea2031d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_train):\n",
    "        self.data_x = torch.tensor(df_train.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d2789f-ca45-4889-9a6d-1aaad9fa6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_x, df_y=None):\n",
    "        self.data_x = torch.tensor(df_x.values, dtype=torch.float32)\n",
    "        self.data_y = torch.tensor(df_y.values, dtype=torch.float32) if df_y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_y is not None:\n",
    "            return self.data_x[idx], self.data_y[idx]\n",
    "        return self.data_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20673e01-3e4d-4dd4-8b43-083590c8ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_tensor(dataset):\n",
    "    \"\"\"\n",
    "    Convert a dataset to a single tensor.\n",
    "    \n",
    "    Args:\n",
    "    dataset: A PyTorch Dataset object\n",
    "    \n",
    "    Returns:\n",
    "    A tensor containing all the features from the dataset\n",
    "    \"\"\"\n",
    "    # Create a DataLoader with batch_size equal to the dataset size\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    \n",
    "    # Get the single batch from the DataLoader\n",
    "    data = next(iter(loader))\n",
    "    \n",
    "    # The first element of data is the features (X)\n",
    "    features = data[0]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471a9222-5c4c-4686-85fb-5e56fa180051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train_x, df_train_y)\n",
    "test_dataset = CustomDataset(df_test_x, df_test_y)\n",
    "\n",
    "batch_size = 256  # You can adjust this value\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# We need to preserve the order of the labels in the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_tensor = dataset_to_tensor(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e371266f-2615-40a4-9af0-5b7075ea6f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2058, 0.2252, 0.3257, 0.1648, 0.2052, 0.2247, 0.1791, 0.1340, 0.1963,\n",
      "        0.1903, 0.1480, 0.1994, 0.2263, 0.2221, 0.2250, 0.2245, 0.2143, 0.1351,\n",
      "        0.1301, 0.1677, 0.1971, 0.1378, 0.2518, 0.2006, 0.1570, 0.2777, 0.1616,\n",
      "        0.1305, 0.1554, 0.3124, 0.2238, 0.1934, 0.2126, 0.2255, 0.2092, 0.2218,\n",
      "        0.1662, 0.1839, 0.1796, 0.1936, 0.3354, 0.1447, 0.2547, 0.1745, 0.2093,\n",
      "        0.2254, 0.2209, 0.1388, 0.2657, 0.3009, 0.3363, 0.2039, 0.2794, 0.1328,\n",
      "        0.1917, 0.1272, 0.2933, 0.1203, 0.2263, 0.2499, 0.2737, 0.2551, 0.2710,\n",
      "        0.2703, 0.2425, 0.2600, 0.2109, 0.2306, 0.2715, 0.4298, 0.2924, 0.2265,\n",
      "        0.1985, 0.1819, 0.1723, 0.1865, 0.1902, 0.1992, 0.2328, 0.3629, 0.1995,\n",
      "        0.1529, 0.2016, 0.2146, 0.1607, 0.1716, 0.1920, 0.2458, 0.2480, 0.1868,\n",
      "        0.2856, 0.1917, 0.2047, 0.3246, 0.1571, 0.2578, 0.2559, 0.3903, 0.1657,\n",
      "        0.1932, 0.1867, 0.1792, 0.3621, 0.2292, 0.2812, 0.1717, 0.1533, 0.2665,\n",
      "        0.3203, 0.3466, 0.2517, 0.1523, 0.1971, 0.2305, 0.2175, 0.2509, 0.1349,\n",
      "        0.1064, 0.1972, 0.1901, 0.2615, 0.1658, 0.1010, 0.2137, 0.2550, 0.2957,\n",
      "        0.2613, 0.2841, 0.1447, 0.1217, 0.1665, 0.1622, 0.2569, 0.1655, 0.1892,\n",
      "        0.1202, 0.2511, 0.1631, 0.2744, 0.1239, 0.3035, 0.3579, 0.1334, 0.1985,\n",
      "        0.2060, 0.2376, 0.1642, 0.1263, 0.1569, 0.1596, 0.2474, 0.1845, 0.2293,\n",
      "        0.3250, 0.1879, 0.1583, 0.1996, 0.2308, 0.1064, 0.1575, 0.1378, 0.3132,\n",
      "        0.2912, 0.1917, 0.1640, 0.2624, 0.1652, 0.1859, 0.2359, 0.3092, 0.1545,\n",
      "        0.2936, 0.1320, 0.2128, 0.1910, 0.2673, 0.2641, 0.1805, 0.2899, 0.2203,\n",
      "        0.2928, 0.1461, 0.1472, 0.1578, 0.2028, 0.2964, 0.1718, 0.1305, 0.2619,\n",
      "        0.3571, 0.1251, 0.1947, 0.2095, 0.2516, 0.1210, 0.1596, 0.1743, 0.2398,\n",
      "        0.1890, 0.2260, 0.3331, 0.2150, 0.1394, 0.2040, 0.2994, 0.2258, 0.2090,\n",
      "        0.2366, 0.1682, 0.2094, 0.1248, 0.1893, 0.1883, 0.1849, 0.2190, 0.1694,\n",
      "        0.1920, 0.1786, 0.1757, 0.2057, 0.1435, 0.1037, 0.1783, 0.2076, 0.1265,\n",
      "        0.2048, 0.2331, 0.1822, 0.5403, 0.2457, 0.1149, 0.1527, 0.2011, 0.1929,\n",
      "        0.1401, 0.2471, 0.2188, 0.2330, 0.2146, 0.1794, 0.2549, 0.2193, 0.2203,\n",
      "        0.2374, 0.2115, 0.4151, 0.2519, 0.2246, 0.2556, 0.2128, 0.2244, 0.2504,\n",
      "        0.1559, 0.1394, 0.1705, 0.1601, 0.3533, 0.2204, 0.3133, 0.1942, 0.2932,\n",
      "        0.3096, 0.2228, 0.2138, 0.3069, 0.2761, 0.2678, 0.1167, 0.2070, 0.1524,\n",
      "        0.2092, 0.2032, 0.2774, 0.1819, 0.2205, 0.2075, 0.2120, 0.2976, 0.2247,\n",
      "        0.1675, 0.1597, 0.3294, 0.4139, 0.1606, 0.2680, 0.2026, 0.1208, 0.2019,\n",
      "        0.1865, 0.1541, 0.3122, 0.2710, 0.1129, 0.1750, 0.2015, 0.3237, 0.2347,\n",
      "        0.1545, 0.2268, 0.1823, 0.1644, 0.1267, 0.2423, 0.1955, 0.3954, 0.1129,\n",
      "        0.1529, 0.2455, 0.2131, 0.1527, 0.2258, 0.2029, 0.2894, 0.2470, 0.1537,\n",
      "        0.1515, 0.1483, 0.2210, 0.3054, 0.2580, 0.1695, 0.2784, 0.2834, 0.1737,\n",
      "        0.2859, 0.1582, 0.1543, 0.2334, 0.3665, 0.2071, 0.1422, 0.1661, 0.2528,\n",
      "        0.2877, 0.1532, 0.2294, 0.1745, 0.2073, 0.2471, 0.1666, 0.2393, 0.2983,\n",
      "        0.2133, 0.1353, 0.1665, 0.1402, 0.1706, 0.1576, 0.1108, 0.1599, 0.1474,\n",
      "        0.2064, 0.2263, 0.1490, 0.3671, 0.1459, 0.1384, 0.1776, 0.1338, 0.2262,\n",
      "        0.1331, 0.1205, 0.1340, 0.1973, 0.1673, 0.1489, 0.0950, 0.2722, 0.1514,\n",
      "        0.3151, 0.3288, 0.1691, 0.1794, 0.2703, 0.2770, 0.1297, 0.3085, 0.2334,\n",
      "        0.1749, 0.2495, 0.2007, 0.1580, 0.1883, 0.1685, 0.3335, 0.1288, 0.2033,\n",
      "        0.1401, 0.2326, 0.2552, 0.1741, 0.2862, 0.1686, 0.1640, 0.1713, 0.3359,\n",
      "        0.2495, 0.1545, 0.2664, 0.2559, 0.2097, 0.1311, 0.1451, 0.2141, 0.1899,\n",
      "        0.1971, 0.2418, 0.3392, 0.2394, 0.1402, 0.1544, 0.3361, 0.1737, 0.1243,\n",
      "        0.2275, 0.2606, 0.1846, 0.1990, 0.2617, 0.2161, 0.1696, 0.3638, 0.2037,\n",
      "        0.2109, 0.3265, 0.2974, 0.1995, 0.1520, 0.1599, 0.1727, 0.1914, 0.1928,\n",
      "        0.2210, 0.1959, 0.1838, 0.1896, 0.2654, 0.1183, 0.1852, 0.2330, 0.2279,\n",
      "        0.2266, 0.1633, 0.2435, 0.1837, 0.1927, 0.1725, 0.3085, 0.2299, 0.2135,\n",
      "        0.2795, 0.1783, 0.2015, 0.1758, 0.2074, 0.1942, 0.3173, 0.2350, 0.2191,\n",
      "        0.1110, 0.1920, 0.2834, 0.1547, 0.2326, 0.2959, 0.1982, 0.1305, 0.2876,\n",
      "        0.2143, 0.1617, 0.1591, 0.2169, 0.1318, 0.1287, 0.1627, 0.2506, 0.1864,\n",
      "        0.2217, 0.3896, 0.3153, 0.2536, 0.1260, 0.3407, 0.4014, 0.1110, 0.2677,\n",
      "        0.1537, 0.5832, 0.1914, 0.1578, 0.2302, 0.2364, 0.3633, 0.2528, 0.4155,\n",
      "        0.8164, 0.5970, 0.7455, 0.7669, 0.6724, 0.6956, 0.6162, 0.6018, 0.4969,\n",
      "        0.5628, 0.5409, 0.6967, 0.8379, 0.8386, 0.7063, 0.6965, 0.6484, 0.6480,\n",
      "        0.8117, 0.8055, 0.8058, 0.5499, 1.4856, 0.6610, 1.4098, 1.4592, 0.6645,\n",
      "        0.8191, 0.8181, 0.5078, 1.4617, 1.3765, 1.4280, 1.4301, 1.4199, 0.5225,\n",
      "        0.2659, 0.2646, 0.2702, 1.4100, 1.3423, 1.3804, 1.3809, 1.3138, 1.3748,\n",
      "        1.3718, 1.3646, 1.3578, 1.3513, 1.3450, 1.3391, 1.3336, 1.3283, 0.2828,\n",
      "        1.0872, 1.0525, 1.0496, 0.2683, 1.0442, 1.0424, 1.0415, 1.0411, 1.0412,\n",
      "        1.0418, 1.0429, 1.0445, 1.0466, 1.0492, 1.0523, 1.0559, 0.4001, 1.0628,\n",
      "        0.4217, 1.0676, 1.0731, 1.0791, 1.0855, 1.0924, 0.5681, 0.5546, 0.6851,\n",
      "        0.4695, 0.2448, 0.4148, 0.4365, 0.3574, 0.1869, 0.4202, 0.6394, 0.8631,\n",
      "        0.2590, 0.6263, 0.8790, 0.8922, 0.8586, 0.8542, 0.8489, 0.8329, 0.8489,\n",
      "        0.5199, 0.2327, 0.3185, 0.5481, 0.6750, 0.6893, 1.4308, 0.7240, 0.7288,\n",
      "        1.4135, 1.4573, 1.4191, 1.4187, 1.4058, 1.4242, 1.3963, 1.3815, 1.3735,\n",
      "        1.3764, 1.3617, 1.3535, 1.3461, 1.3644, 1.3453, 1.3635, 1.3483, 1.3247,\n",
      "        1.2712, 1.2907, 1.3291, 1.3356, 0.4135, 1.3272, 0.7173, 0.4485, 0.6463,\n",
      "        1.2892, 1.2409, 0.6822, 0.6489, 0.6245, 0.9462, 0.9476, 0.3894, 0.6327,\n",
      "        0.4405, 0.6766, 0.1853, 0.3995, 0.2161, 0.3929, 0.3858, 0.3816, 0.1739,\n",
      "        0.1789, 0.2169, 0.4047, 0.3796, 0.2227, 0.2203, 0.3652, 0.3172, 0.6349,\n",
      "        0.6305, 0.6273, 0.6250, 0.6235, 0.2628, 0.2966, 0.4421, 0.1890, 0.3665,\n",
      "        0.4136, 0.4385, 0.3599, 0.1411, 0.9192, 0.9238, 0.9004, 0.8936, 0.2790,\n",
      "        0.4086, 0.8805, 0.9020, 0.5500, 0.5582, 0.5249, 0.4823, 0.8672, 0.7455,\n",
      "        0.5764, 0.4246, 0.1945, 0.4163, 0.5674, 0.6189, 0.4046, 0.4108, 0.4169,\n",
      "        0.3584, 0.6964, 0.6632, 0.4630, 0.3933, 0.4400, 0.4387, 0.2828, 0.7784,\n",
      "        0.7004, 0.2429, 0.7643, 0.4127, 0.3119, 0.2657, 0.7163, 0.2936, 0.3993,\n",
      "        0.1653, 0.4077, 0.4068, 0.4385, 0.1701, 0.2006, 1.0120, 1.0120, 1.0120,\n",
      "        1.0120, 1.0120, 1.0120, 0.1946, 0.3389, 0.5365, 0.4098, 0.4354, 0.4417,\n",
      "        0.2647, 0.5382, 0.5312, 0.2019, 0.3816, 0.4033, 0.4750, 0.1612, 0.3097,\n",
      "        0.7583, 0.7241, 0.7148, 0.7191, 0.7184, 0.2824, 0.3025, 0.3392, 0.3087,\n",
      "        0.2189, 0.2847, 0.4125, 0.3169, 0.7247, 0.4223, 0.2140, 0.4070, 0.3181,\n",
      "        0.6997, 0.9573, 0.9573, 0.9567, 0.9567, 0.8873, 0.2027, 0.3821, 0.8975,\n",
      "        0.8975, 0.9019, 0.9019, 0.7073, 0.6767, 0.6648, 0.6667, 0.2321, 0.2875,\n",
      "        0.2326, 0.6889, 0.2950, 0.3858, 0.3749, 0.6480, 0.4632, 0.4863, 0.4469,\n",
      "        0.4350, 0.4498, 1.4380, 1.4761, 1.4697, 1.4633, 1.3979, 1.3979, 1.3935,\n",
      "        1.3935, 1.4493, 1.3643, 1.3643, 1.3728, 1.3728, 1.3514, 1.3514, 1.3076,\n",
      "        1.3076, 1.4234, 1.3249, 1.3878, 1.3125, 1.3132, 1.3255, 1.3210, 1.3210,\n",
      "        1.3210, 1.3210, 1.2905, 0.3673, 0.3898, 1.3276, 1.3269, 0.5493, 0.5567,\n",
      "        0.5158, 1.3053, 1.3547, 1.3408, 1.3150, 0.6035, 1.4055, 0.4534, 1.4531,\n",
      "        0.4606, 1.5549, 0.4382, 0.9311, 0.9578, 0.9481, 1.5636, 0.9173, 0.8783,\n",
      "        0.8765, 0.9280, 0.9198, 0.9146, 0.4087, 0.6463, 0.6664, 0.3080, 0.6395,\n",
      "        0.5245, 0.3774, 0.5348, 0.4430, 0.5929, 0.5858, 0.5306, 0.2906, 0.4447,\n",
      "        0.8575, 0.3521, 0.3318, 0.5391, 0.3645, 0.4614, 0.6360, 0.3879, 0.4421,\n",
      "        0.2121, 0.5214, 0.5205, 0.4768, 0.6036, 0.4522, 0.5660, 0.5641, 0.1585,\n",
      "        0.3184, 0.2135, 0.4752, 0.3653, 0.5927, 0.4041, 0.4504, 0.4473, 0.5735,\n",
      "        0.4141, 0.1593, 0.5133, 0.5589, 0.5859, 0.5537, 0.3519, 0.6185, 0.6059,\n",
      "        0.2589, 0.6039, 0.5593, 0.5527, 0.5507, 0.1627, 0.3633, 0.4025, 0.5535,\n",
      "        0.3746, 0.5432, 0.2720, 0.4814, 0.6279, 0.5105, 0.6363, 0.6040, 0.5237,\n",
      "        0.5814, 0.4213, 0.6772, 0.5313, 0.3363, 0.4573, 0.6838, 0.4712, 0.4712,\n",
      "        0.5670, 0.6048, 0.5706, 0.5060, 0.5919, 0.4386, 0.7531, 0.4428, 0.2994,\n",
      "        0.3382, 0.3085, 0.3783, 0.2936, 0.5322, 0.6650, 0.5746, 0.7959, 0.8124,\n",
      "        0.8016, 0.7912, 0.7813, 0.2675, 0.1895, 0.5145, 0.3833, 0.5884, 0.2917,\n",
      "        0.4403, 0.2532, 0.4458, 0.6499, 0.7494, 0.7417, 0.7332, 0.5789, 0.3184,\n",
      "        0.3236, 0.6126, 0.6797, 0.7036, 0.1516, 0.2641, 0.5112, 0.6354, 0.4132,\n",
      "        0.4352, 0.3961, 0.3613, 0.6765, 0.7634, 0.1586, 0.7561, 0.6331, 0.5456,\n",
      "        0.5778, 0.4566, 0.4118, 0.2554, 0.2288, 0.1663, 0.3428, 0.4558, 0.3662,\n",
      "        0.2986, 0.4312, 0.1754])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, layer_1, layer_2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_a = nn.Linear(in_dim, layer_1)\n",
    "        self.encoder_b = nn.Linear(layer_1, layer_2)\n",
    "        self.decoder_a = nn.Linear(layer_2, layer_1)\n",
    "        self.decoder_b = nn.Linear(layer_1, in_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.encoder_b(x)\n",
    "        x = self.ReLU(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.decoder_b(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "    def detect_anomaly(self, x, threshold):\n",
    "        with torch.no_grad():\n",
    "            x_pred = self(x)\n",
    "            diff = torch.norm(x_pred - x, dim=1)\n",
    "            return diff > threshold\n",
    "\n",
    "def train_autoencoder(model, train_loader, lr, num_epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_values = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for x_batch, _ in train_loader:  # Ignore y values for autoencoder\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_values.append(avg_loss)\n",
    "    \n",
    "    return loss_values\n",
    "\n",
    "\n",
    "# Assume we have a dataset and dataloader\n",
    "in_dim = df_train_x.shape[1]  \n",
    "layer_1 = 8\n",
    "layer_2 = 4\n",
    "lr = 0.001\n",
    "threshold = 0.5 # Example threshold value\n",
    "\n",
    "model = Autoencoder(in_dim, layer_1, layer_2)\n",
    "\n",
    "# Train the model\n",
    "loss_values = train_autoencoder(model, train_loader, lr)\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = model.detect_anomaly(test_tensor, threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3444d39d-e55f-4925-b20a-7817ec0f5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0339\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0014\n",
      "Epoch 4/5, Loss: 0.0014\n",
      "Epoch 5/5, Loss: 0.0014\n",
      "Threshold set to: 0.2949\n",
      "Accuracy: 0.8943\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, layer_1, layer_2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_a = nn.Linear(in_dim, layer_1)\n",
    "        self.encoder_b = nn.Linear(layer_1, layer_2)\n",
    "        self.decoder_a = nn.Linear(layer_2, layer_1)\n",
    "        self.decoder_b = nn.Linear(layer_1, in_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.encoder_b(x)\n",
    "        x = self.ReLU(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.decoder_b(x)\n",
    "        return x  # Remove ReLU here to allow negative values\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "    def compute_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_pred = self(x)\n",
    "            return torch.norm(x_pred - x, dim=1)\n",
    "    \n",
    "    def detect_anomaly(self, x, threshold):\n",
    "        with torch.no_grad():\n",
    "            reconstruction_errors = self.compute_reconstruction_error(x)\n",
    "            # if the error is greater than t, anomaly is a True boolean value\n",
    "            return reconstruction_errors > threshold\n",
    "\n",
    "def train_autoencoder(model, train_loader, lr, num_epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_values = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for x_batch, _ in train_loader:  # Ignore y values for autoencoder\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_values.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return loss_values\n",
    "\n",
    "def set_threshold(model, train_loader, percentile=95):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in train_loader:\n",
    "            errors = model.compute_reconstruction_error(x_batch)\n",
    "            reconstruction_errors.extend(errors.cpu().numpy())\n",
    "    \n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    return threshold\n",
    "\n",
    "in_dim = df_train_x.shape[1]  # 29 features\n",
    "layer_1 = 15\n",
    "layer_2 = 10\n",
    "lr = 0.001\n",
    "model = Autoencoder(in_dim, layer_1, layer_2)\n",
    "\n",
    "# Train the model\n",
    "loss_values = train_autoencoder(model, train_loader, lr)\n",
    "\n",
    "# Set the threshold\n",
    "threshold = set_threshold(model, train_loader, percentile=95)\n",
    "print(f\"Threshold set to: {threshold:.4f}\")\n",
    "\n",
    "# Detect anomalies\n",
    "model.eval()\n",
    "anomalies = model.detect_anomaly(test_tensor, threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_labels = torch.tensor(df_test_y.values, dtype=torch.long)\n",
    "true_labels = test_labels.numpy()  # Assuming -1 for anomalies, 1 for normal\n",
    "predicted_labels = (~anomalies.cpu().numpy()).astype(int) * 2 - 1  # Convert bool to -1/1\n",
    "accuracy = (true_labels == predicted_labels).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84874ddd-0fb2-4c2a-93b5-eaee77219d38",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f438c9-d231-480e-a340-0cf054eab941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parameters to tune\n",
    "# 1. threshold value\n",
    "# 2. deepness of the encoder\n",
    "# 3. latent space dimensionality\n",
    "thresholds = [0.2, 0.3, 0.4]\n",
    "encoder_depths = [1, 2, 3]\n",
    "latent_dims = [2, 4, 8, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d87a27-b5ac-42bc-95e3-f900f1454300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(in_dim, h_dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        encoder_layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(nn.Linear(latent_dim, h_dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            latent_dim = h_dim\n",
    "        decoder_layers.append(nn.Linear(latent_dim, in_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def compute_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_pred = self(x)\n",
    "            return torch.norm(x_pred - x, dim=1)\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, lr=0.001, num_epochs=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, _ in val_loader:\n",
    "                output = model(x_batch)\n",
    "                val_loss += criterion(output, x_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, threshold):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            errors = model.compute_reconstruction_error(x_batch)\n",
    "            predictions = (errors > threshold).float()\n",
    "            true_labels.extend(y_batch.numpy())\n",
    "            pred_labels.extend(predictions.numpy())\n",
    "    \n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    \n",
    "    accuracy = (true_labels == pred_labels).mean()\n",
    "    return accuracy\n",
    "\n",
    "def hyperparameter_tuning(X_train, y_train, X_test, y_test):\n",
    "    # Define hyperparameter ranges\n",
    "    thresholds = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "    encoder_depths = [1, 2, 3]\n",
    "    latent_dims = [2, 4, 8, 16]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for threshold, depth, latent_dim in product(thresholds, encoder_depths, latent_dims):\n",
    "        print(f\"Testing: threshold={threshold}, depth={depth}, latent_dim={latent_dim}\")\n",
    "        \n",
    "        hidden_dims = [64] * depth  # You can adjust this based on your needs\n",
    "        model = Autoencoder(X_train.shape[1], hidden_dims, latent_dim)\n",
    "        \n",
    "        trained_model = train_autoencoder(model, train_loader, val_loader)\n",
    "        accuracy = evaluate_model(trained_model, test_loader, threshold)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (threshold, depth, latent_dim)\n",
    "    \n",
    "    print(f\"Best parameters: threshold={best_params[0]}, depth={best_params[1]}, latent_dim={best_params[2]}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Usage\n",
    "# Assuming X_train, y_train, X_test, y_test are your data\n",
    "best_params = hyperparameter_tuning(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
