{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9859a5-834e-450e-a264-0fc921290bca",
   "metadata": {},
   "source": [
    "# Case Study 4: Credit Card Fraud Detection\n",
    "Group Members: Benjamin Ang, Harvey Felipe, Enika Maninang, Jeremy Tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e933b74-6842-484d-bd1c-50ecda11680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c369041-f76b-42d1-a4df-ef8738827e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935192</td>\n",
       "      <td>0.766490</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.763439</td>\n",
       "      <td>0.267669</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.475312</td>\n",
       "      <td>0.510600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561184</td>\n",
       "      <td>0.522992</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.391253</td>\n",
       "      <td>0.585122</td>\n",
       "      <td>0.394557</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.312697</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978542</td>\n",
       "      <td>0.770067</td>\n",
       "      <td>0.840298</td>\n",
       "      <td>0.271796</td>\n",
       "      <td>0.766120</td>\n",
       "      <td>0.262192</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.786298</td>\n",
       "      <td>0.453981</td>\n",
       "      <td>0.505267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557840</td>\n",
       "      <td>0.480237</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.446013</td>\n",
       "      <td>0.416345</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935217</td>\n",
       "      <td>0.753118</td>\n",
       "      <td>0.868141</td>\n",
       "      <td>0.268766</td>\n",
       "      <td>0.762329</td>\n",
       "      <td>0.281122</td>\n",
       "      <td>0.270177</td>\n",
       "      <td>0.788042</td>\n",
       "      <td>0.410603</td>\n",
       "      <td>0.513018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565477</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>0.678939</td>\n",
       "      <td>0.289354</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402727</td>\n",
       "      <td>0.415489</td>\n",
       "      <td>0.311911</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941878</td>\n",
       "      <td>0.765304</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.213661</td>\n",
       "      <td>0.765647</td>\n",
       "      <td>0.275559</td>\n",
       "      <td>0.266803</td>\n",
       "      <td>0.789434</td>\n",
       "      <td>0.414999</td>\n",
       "      <td>0.507585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>0.510277</td>\n",
       "      <td>0.662607</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.614245</td>\n",
       "      <td>0.389197</td>\n",
       "      <td>0.417669</td>\n",
       "      <td>0.314371</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938617</td>\n",
       "      <td>0.776520</td>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.269796</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.263984</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.782484</td>\n",
       "      <td>0.490950</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561327</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.401270</td>\n",
       "      <td>0.566343</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.317490</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7   \n",
       "0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815  \\\n",
       "1  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
       "2  0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
       "3  0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
       "4  0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
       "\n",
       "         x8        x9       x10  ...       x21       x22       x23       x24   \n",
       "0  0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793  0.391253  \\\n",
       "1  0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938  0.336440   \n",
       "2  0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939  0.289354   \n",
       "3  0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607  0.223826   \n",
       "4  0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392  0.401270   \n",
       "\n",
       "        x25       x26       x27       x28       x29  y  \n",
       "0  0.585122  0.394557  0.418976  0.312697  0.005824  1  \n",
       "1  0.587290  0.446013  0.416345  0.313423  0.000105  1  \n",
       "2  0.559515  0.402727  0.415489  0.311911  0.014739  1  \n",
       "3  0.614245  0.389197  0.417669  0.314371  0.004807  1  \n",
       "4  0.566343  0.507497  0.420561  0.317490  0.002724  1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"creditcardfraud.csv\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Number of features\n",
    "num_features = df.shape[1]\n",
    "\n",
    "# Create column names: x1, x2, ..., x29\n",
    "column_names = [f\"x{i}\" for i in range(1, num_features )]\n",
    "\n",
    "# Add the target column name 'y'\n",
    "column_names.append(\"y\")\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bebe824-8949-493e-837d-6a5d12430848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1    284315\n",
       "-1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].value_counts()\n",
    "\n",
    "# 1 are the normal, -1 are the anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ec4fda-b1a0-4050-a667-517f1294005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 1\n",
    "df_normal = df[df[\"y\"] == 1]\n",
    "\n",
    "# y = -1\n",
    "df_anomaly = df[df[\"y\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b925a8-16be-4cd7-8a4d-5f411357fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set \n",
    "# 492 1's and 492 -1's\n",
    "\n",
    "df_test_ones = df_normal.sample(492)\n",
    "df_test = pd.concat([df_test_ones, df_anomaly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f6304f-7dbf-4306-b388-cc5a859e3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = df_test.index.tolist()\n",
    "df_train = df.drop(indices_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f56a406-701d-46a1-8d2f-3c416e8dc79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "1    283823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1617fb0-68c9-4af5-929f-efb814123eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       " 1    284315\n",
       "-1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62fff9f0-da05-4f25-b779-c326d680c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = df_train.drop(\"y\", axis = 1)\n",
    "df_train_y = df_train[\"y\"]\n",
    "df_test_x = df_test.drop(\"y\", axis = 1)\n",
    "df_test_y = df_test[\"y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20673e01-3e4d-4dd4-8b43-083590c8ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_x, df_y=None):\n",
    "        self.data_x = torch.tensor(df_x.values, dtype=torch.float32)\n",
    "        self.data_y = torch.tensor(df_y.values, dtype=torch.float32) if df_y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_y is not None:\n",
    "            return self.data_x[idx], self.data_y[idx]\n",
    "        return self.data_x[idx]\n",
    "    \n",
    "def dataset_to_tensor(dataset):\n",
    "    \"\"\"\n",
    "    Convert a dataset to a single tensor.\n",
    "    \n",
    "    Args:\n",
    "    dataset: A PyTorch Dataset object\n",
    "    \n",
    "    Returns:\n",
    "    A tensor containing all the features from the dataset\n",
    "    \"\"\"\n",
    "    # Create a DataLoader with batch_size equal to the dataset size\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    \n",
    "    # Get the single batch from the DataLoader\n",
    "    data = next(iter(loader))\n",
    "    \n",
    "    # The first element of data is the features (X)\n",
    "    features = data[0]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471a9222-5c4c-4686-85fb-5e56fa180051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_train_x, df_train_y)\n",
    "test_dataset = CustomDataset(df_test_x, df_test_y)\n",
    "\n",
    "batch_size = 256  # You can adjust this value\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# We need to preserve the order of the labels in the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_tensor = dataset_to_tensor(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3444d39d-e55f-4925-b20a-7817ec0f5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0155\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0014\n",
      "Epoch 4/5, Loss: 0.0013\n",
      "Epoch 5/5, Loss: 0.0011\n",
      "Threshold set to: 0.2581\n",
      "Accuracy: 0.9075\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, layer_1, layer_2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_a = nn.Linear(in_dim, layer_1)\n",
    "        self.encoder_b = nn.Linear(layer_1, layer_2)\n",
    "        self.decoder_a = nn.Linear(layer_2, layer_1)\n",
    "        self.decoder_b = nn.Linear(layer_1, in_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.encoder_b(x)\n",
    "        x = self.ReLU(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder_a(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.decoder_b(x)\n",
    "        return x  # Remove ReLU here to allow negative values\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "    def compute_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_pred = self(x)\n",
    "            return torch.norm(x_pred - x, dim=1)\n",
    "    \n",
    "    def detect_anomaly(self, x, threshold):\n",
    "        with torch.no_grad():\n",
    "            reconstruction_errors = self.compute_reconstruction_error(x)\n",
    "            # if the error is greater than t, anomaly is a True boolean value\n",
    "            return reconstruction_errors > threshold\n",
    "\n",
    "def train_autoencoder(model, train_loader, lr, num_epochs=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_values = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for x_batch, _ in train_loader:  # Ignore y values for autoencoder\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_values.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def set_threshold(model, train_loader, percentile=95):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in train_loader:\n",
    "            errors = model.compute_reconstruction_error(x_batch)\n",
    "            reconstruction_errors.extend(errors.cpu().numpy())\n",
    "    \n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    return threshold\n",
    "\n",
    "in_dim = df_train_x.shape[1]  # 29 features\n",
    "layer_1 = 15\n",
    "layer_2 = 10\n",
    "lr = 0.001\n",
    "model = Autoencoder(in_dim, layer_1, layer_2)\n",
    "\n",
    "# Train the model\n",
    "loss_values = train_autoencoder(model, train_loader, lr)\n",
    "\n",
    "# Set the threshold\n",
    "threshold = set_threshold(model, train_loader, percentile=95)\n",
    "print(f\"Threshold set to: {threshold:.4f}\")\n",
    "\n",
    "# Detect anomalies\n",
    "model.eval()\n",
    "anomalies = model.detect_anomaly(test_tensor, threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_labels = torch.tensor(df_test_y.values, dtype=torch.long)\n",
    "true_labels = test_labels.numpy()  # Assuming -1 for anomalies, 1 for normal\n",
    "predicted_labels = (~anomalies.cpu().numpy()).astype(int) * 2 - 1  # Convert bool to -1/1\n",
    "accuracy = (true_labels == predicted_labels).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84874ddd-0fb2-4c2a-93b5-eaee77219d38",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d87a27-b5ac-42bc-95e3-f900f1454300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.1, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0155\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0016\n",
      "Epoch 4/5, Loss: 0.0015\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.1, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0133\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.2, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0195\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0017\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.2, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0114\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0016\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.5, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0163\n",
      "Epoch 2/5, Loss: 0.0015\n",
      "Epoch 3/5, Loss: 0.0014\n",
      "Epoch 4/5, Loss: 0.0014\n",
      "Epoch 5/5, Loss: 0.0014\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=5, threshold=0.5, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0093\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0016\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.1, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0138\n",
      "Epoch 2/5, Loss: 0.0013\n",
      "Epoch 3/5, Loss: 0.0012\n",
      "Epoch 4/5, Loss: 0.0011\n",
      "Epoch 5/5, Loss: 0.0010\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.1, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0078\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0016\n",
      "Epoch 5/5, Loss: 0.0014\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.2, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0131\n",
      "Epoch 2/5, Loss: 0.0015\n",
      "Epoch 3/5, Loss: 0.0013\n",
      "Epoch 4/5, Loss: 0.0011\n",
      "Epoch 5/5, Loss: 0.0009\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.2, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0141\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.5, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0183\n",
      "Epoch 2/5, Loss: 0.0013\n",
      "Epoch 3/5, Loss: 0.0010\n",
      "Epoch 4/5, Loss: 0.0009\n",
      "Epoch 5/5, Loss: 0.0008\n",
      "Training Autoencoder with layers=[29, 20], latent_dim=10, threshold=0.5, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0064\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0016\n",
      "Epoch 5/5, Loss: 0.0013\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.1, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0102\n",
      "Epoch 2/5, Loss: 0.0013\n",
      "Epoch 3/5, Loss: 0.0012\n",
      "Epoch 4/5, Loss: 0.0011\n",
      "Epoch 5/5, Loss: 0.0010\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.1, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0158\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0017\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.2, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0129\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0016\n",
      "Epoch 4/5, Loss: 0.0016\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.2, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0135\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0017\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.5, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0225\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0015\n",
      "Epoch 4/5, Loss: 0.0014\n",
      "Epoch 5/5, Loss: 0.0014\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=5, threshold=0.5, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0106\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0017\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.1, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0116\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0014\n",
      "Epoch 4/5, Loss: 0.0014\n",
      "Epoch 5/5, Loss: 0.0012\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.1, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0099\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0017\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.2, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0167\n",
      "Epoch 2/5, Loss: 0.0016\n",
      "Epoch 3/5, Loss: 0.0016\n",
      "Epoch 4/5, Loss: 0.0016\n",
      "Epoch 5/5, Loss: 0.0016\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.2, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0101\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0015\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.5, activation=ReLU\n",
      "Epoch 1/5, Loss: 0.0178\n",
      "Epoch 2/5, Loss: 0.0015\n",
      "Epoch 3/5, Loss: 0.0012\n",
      "Epoch 4/5, Loss: 0.0012\n",
      "Epoch 5/5, Loss: 0.0011\n",
      "Training Autoencoder with layers=[29, 20, 15], latent_dim=10, threshold=0.5, activation=Sigmoid\n",
      "Epoch 1/5, Loss: 0.0122\n",
      "Epoch 2/5, Loss: 0.0017\n",
      "Epoch 3/5, Loss: 0.0017\n",
      "Epoch 4/5, Loss: 0.0017\n",
      "Epoch 5/5, Loss: 0.0016\n",
      "          layers  latent_dim  threshold activation  accuracy\n",
      "0       [29, 20]           5        0.1       ReLU  0.504065\n",
      "1       [29, 20]           5        0.1    Sigmoid  0.518293\n",
      "2       [29, 20]           5        0.2       ReLU  0.732724\n",
      "3       [29, 20]           5        0.2    Sigmoid  0.772358\n",
      "4       [29, 20]           5        0.5       ReLU  0.793699\n",
      "5       [29, 20]           5        0.5    Sigmoid  0.802846\n",
      "6       [29, 20]          10        0.1       ReLU  0.529472\n",
      "7       [29, 20]          10        0.1    Sigmoid  0.515244\n",
      "8       [29, 20]          10        0.2       ReLU  0.885163\n",
      "9       [29, 20]          10        0.2    Sigmoid  0.807927\n",
      "10      [29, 20]          10        0.5       ReLU  0.775407\n",
      "11      [29, 20]          10        0.5    Sigmoid  0.797764\n",
      "12  [29, 20, 15]           5        0.1       ReLU  0.552846\n",
      "13  [29, 20, 15]           5        0.1    Sigmoid  0.502033\n",
      "14  [29, 20, 15]           5        0.2       ReLU  0.777439\n",
      "15  [29, 20, 15]           5        0.2    Sigmoid  0.731707\n",
      "16  [29, 20, 15]           5        0.5       ReLU  0.795732\n",
      "17  [29, 20, 15]           5        0.5    Sigmoid  0.806911\n",
      "18  [29, 20, 15]          10        0.1       ReLU  0.531504\n",
      "19  [29, 20, 15]          10        0.1    Sigmoid  0.501016\n",
      "20  [29, 20, 15]          10        0.2       ReLU  0.751016\n",
      "21  [29, 20, 15]          10        0.2    Sigmoid  0.764228\n",
      "22  [29, 20, 15]          10        0.5       ReLU  0.800813\n",
      "23  [29, 20, 15]          10        0.5    Sigmoid  0.803862\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "layer_depths = [\n",
    "    [in_dim, 20], # number of dimensions in each layer\n",
    "    [in_dim, 20, 15]  # number of dimensions in each layer\n",
    "]\n",
    "latent_space_dims = [5, 10]  \n",
    "thresholds = [0.1, 0.2, 0.4] \n",
    "activations = [nn.ReLU(), nn.Sigmoid()]  # Add this line\n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform grid search\n",
    "for layers, latent_dim, threshold, activation in product(layer_depths, latent_space_dims, thresholds, activations):\n",
    "    print(f\"Training Autoencoder with layers={layers}, latent_dim={latent_dim}, threshold={threshold}, activation={activation.__class__.__name__}\")\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, layers, latent_dim, activation):\n",
    "\n",
    "            # the autoencoder has a depth of 2 layers\n",
    "            if len(layers) == 2:\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(layers[0], layers[1]),\n",
    "                    activation,\n",
    "                    nn.Linear(layers[1], latent_dim),\n",
    "                    activation)\n",
    "                \n",
    "                self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, layers[1]),\n",
    "                activation,\n",
    "                nn.Linear(layers[1], layers[0]))\n",
    "\n",
    "            # the autoencoder has a depth of 3 layers\n",
    "            elif len(layers) == 3:\n",
    "                super(Autoencoder, self).__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(layers[0], layers[1]),\n",
    "                    activation,\n",
    "                    nn.Linear(layers[1], layers[2]),\n",
    "                    activation,\n",
    "                    nn.Linear(layers[2], latent_dim),\n",
    "                    activation\n",
    "            )\n",
    "                self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, layers[2]),\n",
    "                activation,\n",
    "                nn.Linear(layers[2], layers[1]),\n",
    "                activation, \n",
    "                nn.Linear(layers[1], layers[0]))\n",
    "                    \n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "        \n",
    "        def compute_reconstruction_error(self, x):\n",
    "            with torch.no_grad():\n",
    "                x_pred = self(x)\n",
    "                return torch.norm(x_pred - x, dim=1)  \n",
    "    \n",
    "        def detect_anomaly(self, x, threshold):\n",
    "            with torch.no_grad():\n",
    "                reconstruction_errors = self.compute_reconstruction_error(x)\n",
    "                return reconstruction_errors > threshold\n",
    "\n",
    "    model = Autoencoder(layers, latent_dim, activation)\n",
    "    \n",
    "    loss_values = train_autoencoder(model, train_loader, lr)\n",
    "    \n",
    "    model.eval()\n",
    "    anomalies = model.detect_anomaly(test_tensor, threshold)\n",
    "    \n",
    "    predicted_labels = (~anomalies.cpu().numpy()).astype(int) * 2 - 1  # Convert bool to -1/1\n",
    "    accuracy = (true_labels == predicted_labels).mean()\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layers,\n",
    "        'latent_dim': latent_dim,\n",
    "        'threshold': threshold,\n",
    "        'activation': activation.__class__.__name__,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56446982-1c23-4fb5-be23-4602e1f4fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>latent_dim</th>\n",
       "      <th>threshold</th>\n",
       "      <th>activation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.504065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.518293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.732724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.793699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.802846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.529472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.515244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.885163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.807927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.775407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[29, 20]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.797764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.552846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.502033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.777439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.795732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.806911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.531504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.501016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.751016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.800813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[29, 20, 15]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>0.803862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          layers  latent_dim  threshold activation  accuracy\n",
       "0       [29, 20]           5        0.1       ReLU  0.504065\n",
       "1       [29, 20]           5        0.1    Sigmoid  0.518293\n",
       "2       [29, 20]           5        0.2       ReLU  0.732724\n",
       "3       [29, 20]           5        0.2    Sigmoid  0.772358\n",
       "4       [29, 20]           5        0.5       ReLU  0.793699\n",
       "5       [29, 20]           5        0.5    Sigmoid  0.802846\n",
       "6       [29, 20]          10        0.1       ReLU  0.529472\n",
       "7       [29, 20]          10        0.1    Sigmoid  0.515244\n",
       "8       [29, 20]          10        0.2       ReLU  0.885163\n",
       "9       [29, 20]          10        0.2    Sigmoid  0.807927\n",
       "10      [29, 20]          10        0.5       ReLU  0.775407\n",
       "11      [29, 20]          10        0.5    Sigmoid  0.797764\n",
       "12  [29, 20, 15]           5        0.1       ReLU  0.552846\n",
       "13  [29, 20, 15]           5        0.1    Sigmoid  0.502033\n",
       "14  [29, 20, 15]           5        0.2       ReLU  0.777439\n",
       "15  [29, 20, 15]           5        0.2    Sigmoid  0.731707\n",
       "16  [29, 20, 15]           5        0.5       ReLU  0.795732\n",
       "17  [29, 20, 15]           5        0.5    Sigmoid  0.806911\n",
       "18  [29, 20, 15]          10        0.1       ReLU  0.531504\n",
       "19  [29, 20, 15]          10        0.1    Sigmoid  0.501016\n",
       "20  [29, 20, 15]          10        0.2       ReLU  0.751016\n",
       "21  [29, 20, 15]          10        0.2    Sigmoid  0.764228\n",
       "22  [29, 20, 15]          10        0.5       ReLU  0.800813\n",
       "23  [29, 20, 15]          10        0.5    Sigmoid  0.803862"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6da3f63-3256-4553-b5e1-9207e6d46654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers        [29, 20]\n",
       "latent_dim          10\n",
       "threshold          0.2\n",
       "activation        ReLU\n",
       "accuracy      0.885163\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858082b-686f-463c-95a7-60495a4967e1",
   "metadata": {},
   "source": [
    "## Guide Questions:\n",
    "1. What was the optimal t value in your group's case? (example: was optimal t more conservative compared to the other t values. If so, why do you think this is the case for something like credit card fraud detection?)\r\n",
    "2. \n",
    "What was the optimal deepness for an autoencoder in your case? Why do you think this value worked well?\n",
    "3. \r\n",
    "What was the optimal latent space dimensionality in your case compared to the other latent space dimensionality values? Why do you think this value worked well?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016eb7d-ad10-4302-8ac0-7a36080b4efe",
   "metadata": {},
   "source": [
    "## Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53d1d8ea-ec82-4a08-bfb8-21dd65525258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threshold\n",
       "0.1    0.519309\n",
       "0.2    0.777820\n",
       "0.5    0.797129\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby('threshold')['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9528b-7915-433d-92ce-618a0b9bb90c",
   "metadata": {},
   "source": [
    "1. When taking the average accuracy for all the autoencoders, the models with a threshold of 0.2 and 0.5 have a significantly higher average accuracy than the models with a threshold of 0.1. While the average accuracy of the models with a threshold of 0.5 is slightly higher than the models with a threshold of 0.2, the best model had a threshold of 0.2. This was because a moderatively conservative threshold such as 0.2 balances the likelihood of the model classifying false positives and false negatives. A less conservative threshold such as 0.1 will result in the model classifying too many legitimate credit card transactions as fraudulent (false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d020f54-6d3d-4f8a-95da-cb7779b42f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers_str\n",
       "[29, 20, 15]    0.693259\n",
       "[29, 20]        0.702913\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['layers_str'] = results_df['layers'].apply(str)\n",
    "results_df.groupby('layers_str')['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98cffa-e475-43ae-b26a-f18fe68512a6",
   "metadata": {},
   "source": [
    "2. On average, the shallow autoencoder with two hidden layers, one in the encoder and decoder respectively, had a higher average accuracy score compared to the deeper autoencoder. The best performing model also had only two hidden layers. Since the model has fewer parameters and layers, the autoencoder is less likely to overfit. Moreover, the dataset used might not require complex feature extraction or dimensional reductionability so a simpler model was able to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfa41684-19ab-4545-a8c8-4f30d61e7bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "latent_dim\n",
       "5     0.690888\n",
       "10    0.705285\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby('latent_dim')['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221efddb-e000-4d4c-9431-eeca876bef65",
   "metadata": {},
   "source": [
    "3. A latent space of 10 resulted in a higher average accuracy compared to a latent space of 5. For the dataset used in the context of anomaly detection, the data may be better represented in 10 dimensions compared to being overcompressed in 5 dimensions. Moreover, less information is lost when the autoencoder compresses the data from 20 or 15 dimensions to 10 dimensions as compared to when the data is compressed directly to 5 dimensions before it is subsequently reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0f547-89e5-4b72-8c86-6de0069d85a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
