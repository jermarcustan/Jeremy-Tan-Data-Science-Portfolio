{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "158a0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from random import seed\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52562227",
   "metadata": {},
   "source": [
    "## 1. Using only rainfall and water level inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce6dea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86070146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Rainfall_Aries</th>\n",
       "      <th>Rainfall_Boso</th>\n",
       "      <th>Rainfall_Campana</th>\n",
       "      <th>Rainfall_Nangka</th>\n",
       "      <th>Rainfall_Oro</th>\n",
       "      <th>Waterlevel_Sto_Nino</th>\n",
       "      <th>Waterlevel_Montalban</th>\n",
       "      <th>Discharge_Sto_Nino</th>\n",
       "      <th>Discharge_San_Jose</th>\n",
       "      <th>Cross_Section_Sto_Nino</th>\n",
       "      <th>Cross_Section_Montalban</th>\n",
       "      <th>Velocity_Sto_Nino</th>\n",
       "      <th>Velocity_Montalban</th>\n",
       "      <th>datetime</th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.033407</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>803.88</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.19</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.280072</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>804.54</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.19</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.280072</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>804.54</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.529056</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.20</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.529056</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.20</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.529056</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.20</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 05:00:00</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.21</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.780375</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.86</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 06:00:00</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.21</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.780375</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.86</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 07:00:00</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.21</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.780375</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.86</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 08:00:00</td>\n",
       "      <td>28800.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.21</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.780375</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.86</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 09:00:00</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>14420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index  Rainfall_Aries  Rainfall_Boso  Rainfall_Campana  \\\n",
       "0           0      0               0              1                 2   \n",
       "1           1      1               0              1                 1   \n",
       "2           2      2               1              1                 1   \n",
       "3           3      3               0              0                 0   \n",
       "4           4      4               1              1                 1   \n",
       "5           5      5               0              0                 1   \n",
       "6           6      6               0              1                 1   \n",
       "7           7      7               0              0                 0   \n",
       "8           8      8               1              0                 1   \n",
       "9           9      9               0              0                 0   \n",
       "\n",
       "   Rainfall_Nangka  Rainfall_Oro  Waterlevel_Sto_Nino  Waterlevel_Montalban  \\\n",
       "0                0             0                12.18                 21.03   \n",
       "1                1             0                12.19                 21.03   \n",
       "2                0             1                12.19                 21.03   \n",
       "3                1             0                12.20                 21.03   \n",
       "4                0             0                12.20                 21.03   \n",
       "5                0             0                12.20                 21.03   \n",
       "6                0             0                12.21                 21.03   \n",
       "7                0             0                12.21                 21.03   \n",
       "8                0             0                12.21                 21.03   \n",
       "9                0             0                12.21                 21.03   \n",
       "\n",
       "   Discharge_Sto_Nino  Discharge_San_Jose  Cross_Section_Sto_Nino  \\\n",
       "0           21.033407           14.842428                  803.88   \n",
       "1           21.280072           14.842428                  804.54   \n",
       "2           21.280072           14.842428                  804.54   \n",
       "3           21.529056           14.842428                  805.20   \n",
       "4           21.529056           14.842428                  805.20   \n",
       "5           21.529056           14.842428                  805.20   \n",
       "6           21.780375           14.842428                  805.86   \n",
       "7           21.780375           14.842428                  805.86   \n",
       "8           21.780375           14.842428                  805.86   \n",
       "9           21.780375           14.842428                  805.86   \n",
       "\n",
       "   Cross_Section_Montalban  Velocity_Sto_Nino  Velocity_Montalban  \\\n",
       "0                    630.9           0.026165            0.023526   \n",
       "1                    630.9           0.026450            0.023526   \n",
       "2                    630.9           0.026450            0.023526   \n",
       "3                    630.9           0.026738            0.023526   \n",
       "4                    630.9           0.026738            0.023526   \n",
       "5                    630.9           0.026738            0.023526   \n",
       "6                    630.9           0.027027            0.023526   \n",
       "7                    630.9           0.027027            0.023526   \n",
       "8                    630.9           0.027027            0.023526   \n",
       "9                    630.9           0.027027            0.023526   \n",
       "\n",
       "              datetime        t      x  \n",
       "0  2016-01-01 00:00:00      0.0  14420  \n",
       "1  2016-01-01 01:00:00   3600.0  14420  \n",
       "2  2016-01-01 02:00:00   7200.0  14420  \n",
       "3  2016-01-01 03:00:00  10800.0  14420  \n",
       "4  2016-01-01 04:00:00  14400.0  14420  \n",
       "5  2016-01-01 05:00:00  18000.0  14420  \n",
       "6  2016-01-01 06:00:00  21600.0  14420  \n",
       "7  2016-01-01 07:00:00  25200.0  14420  \n",
       "8  2016-01-01 08:00:00  28800.0  14420  \n",
       "9  2016-01-01 09:00:00  32400.0  14420  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"compiled_data_2016_2017.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7df159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'Waterlevel_Sto_Nino']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for time series: split into 50-25-25\n",
    "n = len(df)\n",
    "train_df = df[0:int(n*0.5)]\n",
    "val_df = df[int(n*0.5):int(n*0.75)]\n",
    "test_df = df[int(n*0.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    # Creating tf datasets for more convenient use and integration into model in the future\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    # properties to access them as tf datasets\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wide window uses independent hours of data as input to predict the water level of the next hour\n",
    "# Here, the prediction is done on 6 hours\n",
    "# This is used for Dense and Recurrent Neural Networks\n",
    "wide_window = WindowGenerator(\n",
    "        input_width=6, label_width=6, shift=1,\n",
    "        label_columns=['Waterlevel_Sto_Nino']\n",
    "    )\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conv window is used for the Convolutional Neural Netwrok\n",
    "# 6 consecutive hours of data are used together to make predictions one hour into the future\n",
    "CONV_WIDTH = 6\n",
    "conv_window = WindowGenerator(\n",
    "        input_width=CONV_WIDTH,\n",
    "        label_width=1,\n",
    "        shift=1,\n",
    "        label_columns=['Waterlevel_Sto_Nino']\n",
    "    )\n",
    "\n",
    "conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38dda971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.square(K.sum(xm * ym))\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = (x_square_sum * y_square_sum) + K.epsilon()\n",
    "    \n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0a6f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NSE(y_true, y_pred):\n",
    "    '''\n",
    "    This is the Nash-Sutcliffe Efficiency Coefficient\n",
    "    '''\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    y_true = K.flatten(y_true)\n",
    "\n",
    "    \n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy compiling and fitting of different models\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(), \n",
    "        optimizer='adam', \n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        window.train, \n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=window.val,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Neural Network\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Convolution Neural Network\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=(CONV_WIDTH,), activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "# LSTM\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_history = compile_and_fit(dense, wide_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89058b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history = compile_and_fit(conv_model, conv_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = compile_and_fit(lstm_model, wide_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['Dense'] = dense.evaluate(wide_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['Dense'] = dense.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ff473",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9714239",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1daa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80461f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df_val = pd.DataFrame.from_dict(performance, orient='index', columns=['Loss', 'MSE', 'NSE', 'R^2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d7c95",
   "metadata": {},
   "source": [
    "## 2. Univariate Time series ANN - Using only water level \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df):\n",
    "        # Store the raw data - select only the Sto Nino water level column\n",
    "        self.train_df = train_df[['Waterlevel_Sto_Nino']].values\n",
    "        self.val_df = val_df[['Waterlevel_Sto_Nino']].values\n",
    "        self.test_df = test_df[['Waterlevel_Sto_Nino']].values\n",
    "\n",
    "        # Work out the window parameters\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        # Input and label slices\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        # Features shape is (batch, time_steps, 1)\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        \n",
    "        # Set shapes explicitly\n",
    "        inputs.set_shape([None, self.input_width, 1])\n",
    "        labels.set_shape([None, self.label_width, 1])\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        # Ensure data is float32 and has correct shape (samples, 1)\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "            \n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "        \n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "    \n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            result = next(iter(self.train))\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wide window uses independent hours of data as input to predict the water level of the next hour\n",
    "# Here, the prediction is done on 6 hours\n",
    "# This is used for Dense and Recurrent Neural Networks\n",
    "wide_window = WindowGenerator(\n",
    "        input_width=6, label_width=6, shift=1,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        test_df=test_df\n",
    "    )\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79509fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conv window is used for the Convolutional Neural Netwrok\n",
    "# 6 consecutive hours of data are used together to make predictions one hour into the future\n",
    "CONV_WIDTH = 6\n",
    "conv_window = WindowGenerator(\n",
    "        input_width=CONV_WIDTH,\n",
    "        label_width=1,\n",
    "        shift=1,\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        test_df=test_df\n",
    "    )\n",
    "\n",
    "conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Neural Network\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=[wide_window.input_width, 1]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Convolution Neural Network\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=(CONV_WIDTH,), activation='relu', input_shape=[conv_window.input_width, 1]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "# LSTM\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[wide_window.input_width, 1]),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.square(K.sum(xm * ym))\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = (x_square_sum * y_square_sum) + K.epsilon()\n",
    "    \n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15204afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NSE(y_true, y_pred):\n",
    "    '''\n",
    "    This is the Nash-Sutcliffe Efficiency Coefficient\n",
    "    '''\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    y_true = K.flatten(y_true)\n",
    "\n",
    "    \n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy compiling and fitting of different models\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(), \n",
    "        optimizer='adam', \n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        window.train, \n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=window.val,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_history = compile_and_fit(dense, wide_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history = compile_and_fit(conv_model, conv_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ce615",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = compile_and_fit(lstm_model, wide_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94518b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['Dense'] = dense.evaluate(wide_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4eb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['Dense'] = dense.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca971644",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ed987",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(performance, orient='index', columns=['Loss', 'MSE', 'NSE', 'R^2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "# Get the last input sequence from the test dataset\n",
    "def get_last_input_sequence(test_data, input_width):\n",
    "    \"\"\"\n",
    "    Extracts the last input sequence for prediction\n",
    "    \n",
    "    Args:\n",
    "    - test_data: Test dataset array\n",
    "    - input_width: Number of time steps for input\n",
    "    \n",
    "    Returns:\n",
    "    - Last input sequence reshaped for model prediction\n",
    "    \"\"\"\n",
    "    last_sequence = test_data[-input_width:]\n",
    "    return last_sequence.reshape(1, input_width, 1)\n",
    "\n",
    "# Predict future steps\n",
    "def predict_future(model, initial_input, num_steps):\n",
    "    \"\"\"\n",
    "    Predict future time steps recursively\n",
    "    \n",
    "    Args:\n",
    "    - model: Trained Keras model\n",
    "    - initial_input: Initial input sequence (shape: [1, input_width, 1])\n",
    "    - num_steps: Number of future time steps to predict\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted future values\n",
    "    \"\"\"\n",
    "    current_input = initial_input\n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        # Predict next time step\n",
    "        prediction = model.predict(current_input)\n",
    "        print(prediction[0,-1,0])\n",
    "        \n",
    "        # Append prediction\n",
    "        predictions.append(prediction[0, -1, 0])\n",
    "\n",
    "        # Slide window: remove oldest input, append new prediction\n",
    "        current_input = np.roll(current_input, -1, axis=1)\n",
    "\n",
    "        current_input[0, -1, 0] = prediction[0, -1, 0]\n",
    "\n",
    "        \n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Assuming test_df is your test dataframe and wide_window is your WindowGenerator\n",
    "last_input = get_last_input_sequence(wide_window.test_df, input_width=6)\n",
    "future_predictions = predict_future(lstm_model, last_input, num_steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92067f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5000115",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Waterlevel_Sto_Nino']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856bed8",
   "metadata": {},
   "source": [
    "## 3. ANN with inputs: rainfall, manning's coefficient, bed slope, time, discharge \n",
    "\n",
    "outputs: water level in sto nino, water velocity in sto nino \n",
    "\n",
    "Note: window generator was not used\n",
    "\n",
    "results 4.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40b7798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7730605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_2017 = pd.read_csv(\"compiled_data_2016_2017.csv\")\n",
    "\n",
    "df_2016_2017['friction_coeff'] = [0.033 for i in range(len(df_2016_2017))]\n",
    "df_2016_2017['slope'] = [1/1500 for i in range(len(df_2016_2017))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fb085b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Rainfall_Aries</th>\n",
       "      <th>Rainfall_Boso</th>\n",
       "      <th>Rainfall_Campana</th>\n",
       "      <th>Rainfall_Nangka</th>\n",
       "      <th>Rainfall_Oro</th>\n",
       "      <th>Waterlevel_Sto_Nino</th>\n",
       "      <th>Waterlevel_Montalban</th>\n",
       "      <th>Discharge_Sto_Nino</th>\n",
       "      <th>Discharge_San_Jose</th>\n",
       "      <th>Cross_Section_Sto_Nino</th>\n",
       "      <th>Cross_Section_Montalban</th>\n",
       "      <th>Velocity_Sto_Nino</th>\n",
       "      <th>Velocity_Montalban</th>\n",
       "      <th>datetime</th>\n",
       "      <th>t</th>\n",
       "      <th>x</th>\n",
       "      <th>friction_coeff</th>\n",
       "      <th>slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.18</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.033407</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>803.88</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026165</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.19</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.280072</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>804.54</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.19</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.280072</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>804.54</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026450</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 02:00:00</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.529056</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.20</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.20</td>\n",
       "      <td>21.03</td>\n",
       "      <td>21.529056</td>\n",
       "      <td>14.842428</td>\n",
       "      <td>805.20</td>\n",
       "      <td>630.9</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>0.023526</td>\n",
       "      <td>2016-01-01 04:00:00</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17515</th>\n",
       "      <td>17515</td>\n",
       "      <td>16059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>21.18</td>\n",
       "      <td>28.244204</td>\n",
       "      <td>17.224575</td>\n",
       "      <td>821.04</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>2017-12-31 19:00:00</td>\n",
       "      <td>63140400.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17516</th>\n",
       "      <td>17516</td>\n",
       "      <td>16424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>21.18</td>\n",
       "      <td>28.244204</td>\n",
       "      <td>17.224575</td>\n",
       "      <td>821.04</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>2017-12-31 20:00:00</td>\n",
       "      <td>63144000.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17517</th>\n",
       "      <td>17517</td>\n",
       "      <td>16789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>21.18</td>\n",
       "      <td>28.244204</td>\n",
       "      <td>17.224575</td>\n",
       "      <td>821.04</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>2017-12-31 21:00:00</td>\n",
       "      <td>63147600.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17518</th>\n",
       "      <td>17518</td>\n",
       "      <td>17154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>21.18</td>\n",
       "      <td>28.244204</td>\n",
       "      <td>17.224575</td>\n",
       "      <td>821.04</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>2017-12-31 22:00:00</td>\n",
       "      <td>63151200.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17519</th>\n",
       "      <td>17519</td>\n",
       "      <td>17519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>21.18</td>\n",
       "      <td>28.244204</td>\n",
       "      <td>17.224575</td>\n",
       "      <td>821.04</td>\n",
       "      <td>635.4</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>63154800.0</td>\n",
       "      <td>14420</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17520 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  index  Rainfall_Aries  Rainfall_Boso  Rainfall_Campana  \\\n",
       "0               0      0               0              1                 2   \n",
       "1               1      1               0              1                 1   \n",
       "2               2      2               1              1                 1   \n",
       "3               3      3               0              0                 0   \n",
       "4               4      4               1              1                 1   \n",
       "...           ...    ...             ...            ...               ...   \n",
       "17515       17515  16059               0              0                 0   \n",
       "17516       17516  16424               0              0                 0   \n",
       "17517       17517  16789               0              0                 0   \n",
       "17518       17518  17154               0              0                 0   \n",
       "17519       17519  17519               0              0                 0   \n",
       "\n",
       "       Rainfall_Nangka  Rainfall_Oro  Waterlevel_Sto_Nino  \\\n",
       "0                    0             0                12.18   \n",
       "1                    1             0                12.19   \n",
       "2                    0             1                12.19   \n",
       "3                    1             0                12.20   \n",
       "4                    0             0                12.20   \n",
       "...                ...           ...                  ...   \n",
       "17515                0             0                12.44   \n",
       "17516                0             0                12.44   \n",
       "17517                0             0                12.44   \n",
       "17518                0             0                12.44   \n",
       "17519                0             0                12.44   \n",
       "\n",
       "       Waterlevel_Montalban  Discharge_Sto_Nino  Discharge_San_Jose  \\\n",
       "0                     21.03           21.033407           14.842428   \n",
       "1                     21.03           21.280072           14.842428   \n",
       "2                     21.03           21.280072           14.842428   \n",
       "3                     21.03           21.529056           14.842428   \n",
       "4                     21.03           21.529056           14.842428   \n",
       "...                     ...                 ...                 ...   \n",
       "17515                 21.18           28.244204           17.224575   \n",
       "17516                 21.18           28.244204           17.224575   \n",
       "17517                 21.18           28.244204           17.224575   \n",
       "17518                 21.18           28.244204           17.224575   \n",
       "17519                 21.18           28.244204           17.224575   \n",
       "\n",
       "       Cross_Section_Sto_Nino  Cross_Section_Montalban  Velocity_Sto_Nino  \\\n",
       "0                      803.88                    630.9           0.026165   \n",
       "1                      804.54                    630.9           0.026450   \n",
       "2                      804.54                    630.9           0.026450   \n",
       "3                      805.20                    630.9           0.026738   \n",
       "4                      805.20                    630.9           0.026738   \n",
       "...                       ...                      ...                ...   \n",
       "17515                  821.04                    635.4           0.034401   \n",
       "17516                  821.04                    635.4           0.034401   \n",
       "17517                  821.04                    635.4           0.034401   \n",
       "17518                  821.04                    635.4           0.034401   \n",
       "17519                  821.04                    635.4           0.034401   \n",
       "\n",
       "       Velocity_Montalban             datetime           t      x  \\\n",
       "0                0.023526  2016-01-01 00:00:00         0.0  14420   \n",
       "1                0.023526  2016-01-01 01:00:00      3600.0  14420   \n",
       "2                0.023526  2016-01-01 02:00:00      7200.0  14420   \n",
       "3                0.023526  2016-01-01 03:00:00     10800.0  14420   \n",
       "4                0.023526  2016-01-01 04:00:00     14400.0  14420   \n",
       "...                   ...                  ...         ...    ...   \n",
       "17515            0.027108  2017-12-31 19:00:00  63140400.0  14420   \n",
       "17516            0.027108  2017-12-31 20:00:00  63144000.0  14420   \n",
       "17517            0.027108  2017-12-31 21:00:00  63147600.0  14420   \n",
       "17518            0.027108  2017-12-31 22:00:00  63151200.0  14420   \n",
       "17519            0.027108  2017-12-31 23:00:00  63154800.0  14420   \n",
       "\n",
       "       friction_coeff     slope  \n",
       "0               0.033  0.000667  \n",
       "1               0.033  0.000667  \n",
       "2               0.033  0.000667  \n",
       "3               0.033  0.000667  \n",
       "4               0.033  0.000667  \n",
       "...               ...       ...  \n",
       "17515           0.033  0.000667  \n",
       "17516           0.033  0.000667  \n",
       "17517           0.033  0.000667  \n",
       "17518           0.033  0.000667  \n",
       "17519           0.033  0.000667  \n",
       "\n",
       "[17520 rows x 20 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2016_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c36816a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2016_2017 = df_2016_2017[:int(0.50*len(df_2016_2017))]\n",
    "val_2016_2017 = df_2016_2017[int(0.50*len(df_2016_2017)):int(0.75*len(df_2016_2017))]\n",
    "test_2016_2017 = df_2016_2017[int(0.75*len(df_2016_2017)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "162a9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all inputs\n",
    "X_train_2016_2017 = np.array(train_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x' ,'t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "X_val_2016_2017 = np.array(val_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "X_test_2016_2017 = np.array(test_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "Y_train_2016_2017 = np.array(train_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())\n",
    "Y_val_2016_2017 = np.array(val_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())\n",
    "Y_test_2016_2017 = np.array(test_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "713746be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 2)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_2016_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb9c35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_WIDTH = 6\n",
    "# Dense Neural Network\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# Convolution Neural Network\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=(CONV_WIDTH,), activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "# LSTM\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84e54c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    velocity_loss = tf.keras.losses.mean_squared_error(y_true[:, 0], y_pred[:, 0])\n",
    "    waterlevel_loss = tf.keras.losses.mean_squared_error(y_true[:, 1], y_pred[:, 1])\n",
    "    return velocity_loss + waterlevel_loss  # or any other combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f30e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dnn_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a Deep Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array-like): Training input data\n",
    "    y_train (array-like): Training target data\n",
    "    X_val (array-like): Validation input data\n",
    "    y_val (array-like): Validation target data\n",
    "    max_epochs (int): Maximum number of training epochs\n",
    "    patience (int): Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, history)\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='linear')  # 2 outputs: velocity and water level\n",
    "    ])\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "affe15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_cnn_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    # Reshape input for 1D CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    \n",
    "    conv_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(10, 1)),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=6, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=2)\n",
    "    ])\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    conv_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    conv_history = conv_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return conv_model, conv_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9d651284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_lstm_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a Deep Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array-like): Training input data\n",
    "    y_train (array-like): Training target data\n",
    "    X_val (array-like): Validation input data\n",
    "    y_val (array-like): Validation target data\n",
    "    max_epochs (int): Maximum number of training epochs\n",
    "    patience (int): Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, history)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(X_train.shape) == 2:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    if len(X_val.shape) == 2:\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "    \n",
    "        \n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),  # Add Flatten layer to handle dimension mismatch\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    lstm_history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return lstm_model, lstm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8b4ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 2s 2ms/step - loss: 97851162624.0000 - mean_squared_error: 97851162624.0000 - NSE: -2536450560.0000 - r_square: 0.1624 - val_loss: 4152266.2500 - val_mean_squared_error: 4152266.2500 - val_NSE: -117938.8594 - val_r_square: 0.0148\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 1485402.2500 - mean_squared_error: 1485402.2500 - NSE: -38811.5781 - r_square: 0.0411 - val_loss: 5970993.0000 - val_mean_squared_error: 5970993.0000 - val_NSE: -169467.7812 - val_r_square: 0.0132\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 1458505.8750 - mean_squared_error: 1458505.8750 - NSE: -38101.3984 - r_square: 0.0412 - val_loss: 3443729.5000 - val_mean_squared_error: 3443729.5000 - val_NSE: -97841.9297 - val_r_square: 0.0168\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 1458348.7500 - mean_squared_error: 1458348.7500 - NSE: -38091.1914 - r_square: 0.0430 - val_loss: 2317349.0000 - val_mean_squared_error: 2317349.0000 - val_NSE: -65895.8516 - val_r_square: 0.0156\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 1617556.3750 - mean_squared_error: 1617556.3750 - NSE: -42210.4062 - r_square: 0.0456 - val_loss: 2428908.7500 - val_mean_squared_error: 2428908.7500 - val_NSE: -69050.6094 - val_r_square: 0.0176\n",
      "Epoch 6/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 1317227.6250 - mean_squared_error: 1317227.6250 - NSE: -34431.1836 - r_square: 0.0446 - val_loss: 7655985.0000 - val_mean_squared_error: 7655985.0000 - val_NSE: -217138.1094 - val_r_square: 0.0119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "dnn_model, dnn_history = create_train_dnn_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6e8056f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 2s 3ms/step - loss: 2700339200.0000 - mean_squared_error: 2700339200.0000 - NSE: -70116400.0000 - r_square: 0.2346 - val_loss: 73398.1641 - val_mean_squared_error: 73398.1641 - val_NSE: -2070.6479 - val_r_square: 0.0060\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 401242.9062 - mean_squared_error: 401242.9062 - NSE: -10380.2891 - r_square: 0.2970 - val_loss: 9855.9053 - val_mean_squared_error: 9855.9053 - val_NSE: -278.9723 - val_r_square: 0.0454\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 7824.5405 - mean_squared_error: 7824.5405 - NSE: -203.1162 - r_square: 0.3363 - val_loss: 11610.0244 - val_mean_squared_error: 11610.0244 - val_NSE: -328.4637 - val_r_square: 0.0469\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 3925.5723 - mean_squared_error: 3925.5723 - NSE: -101.3924 - r_square: 0.3419 - val_loss: 1557.0587 - val_mean_squared_error: 1557.0587 - val_NSE: -43.3810 - val_r_square: 0.0220\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 114319.9922 - mean_squared_error: 114319.9922 - NSE: -2957.5469 - r_square: 0.2114 - val_loss: 15165.0156 - val_mean_squared_error: 15165.0156 - val_NSE: -428.8042 - val_r_square: 0.0478\n",
      "Epoch 6/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 2326349.5000 - mean_squared_error: 2326349.5000 - NSE: -60624.5234 - r_square: 0.2335 - val_loss: 24049692.0000 - val_mean_squared_error: 24049692.0000 - val_NSE: -679676.3750 - val_r_square: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "cnn_model, cnn_history = create_train_cnn_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "abf70f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8760, 10)\n",
      "Y_train shape: (8760, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train_2016_2017.shape)\n",
    "print(\"Y_train shape:\", Y_train_2016_2017.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d45c4f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 3s 5ms/step - loss: 8.5357 - mean_squared_error: 8.5357 - NSE: 0.7777 - r_square: 0.0022 - val_loss: 0.1313 - val_mean_squared_error: 0.1313 - val_NSE: 0.9962 - val_r_square: 1.0573e-12\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2351 - mean_squared_error: 0.2351 - NSE: 0.9939 - r_square: 0.0038 - val_loss: 0.0893 - val_mean_squared_error: 0.0893 - val_NSE: 0.9975 - val_r_square: 4.8293e-07\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2327 - mean_squared_error: 0.2327 - NSE: 0.9940 - r_square: 2.3889e-04 - val_loss: 0.0715 - val_mean_squared_error: 0.0715 - val_NSE: 0.9980 - val_r_square: 2.6867e-12\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2303 - mean_squared_error: 0.2303 - NSE: 0.9940 - r_square: 2.4332e-04 - val_loss: 0.1788 - val_mean_squared_error: 0.1788 - val_NSE: 0.9949 - val_r_square: 1.2194e-07\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2312 - mean_squared_error: 0.2312 - NSE: 0.9940 - r_square: 2.3344e-04 - val_loss: 0.0809 - val_mean_squared_error: 0.0809 - val_NSE: 0.9977 - val_r_square: 5.5998e-13\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model, lstm_history = create_train_lstm_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e595369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 22348358.0000 - mean_squared_error: 22348358.0000 - NSE: -564409.0000 - r_square: 0.0569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22348358.0, 22348358.0, -564409.0, 0.05692477896809578]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "313504b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 46835832.0000 - mean_squared_error: 46835832.0000 - NSE: -1183620.5000 - r_square: 0.0553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[46835832.0, 46835832.0, -1183620.5, 0.055303093045949936]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2016_2017 = X_test_2016_2017.reshape(X_test_2016_2017.shape[0], X_test_2016_2017.shape[1], 1)\n",
    "\n",
    "cnn_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "22b187dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.2298 - mean_squared_error: 0.2298 - NSE: 0.9949 - r_square: 3.7171e-13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22978727519512177,\n",
       " 0.22978727519512177,\n",
       " 0.9948520064353943,\n",
       " 3.7171481170883425e-13]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2016_2017 = np.array(test_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "if len(X_test_2016_2017.shape) == 2:\n",
    "    X_test_2016_2017 = X_test_2016_2017.reshape((X_test_2016_2017.shape[0], 1, X_test_2016_2017.shape[1]))\n",
    "lstm_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68558d",
   "metadata": {},
   "source": [
    "### Without \"t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f6fcec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2016_2017 = np.array(train_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x' , 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "X_val_2016_2017 = np.array(val_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "X_test_2016_2017 = np.array(test_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "Y_train_2016_2017 = np.array(train_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())\n",
    "Y_val_2016_2017 = np.array(val_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())\n",
    "Y_test_2016_2017 = np.array(test_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "65e6f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dnn_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a Deep Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array-like): Training input data\n",
    "    y_train (array-like): Training target data\n",
    "    X_val (array-like): Validation input data\n",
    "    y_val (array-like): Validation target data\n",
    "    max_epochs (int): Maximum number of training epochs\n",
    "    patience (int): Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, history)\n",
    "    \"\"\"\n",
    "    # Create the model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='linear')  # 2 outputs: velocity and water level\n",
    "    ])\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "052cd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_cnn_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    # Reshape input for 1D CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    \n",
    "    conv_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(9, 1)),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=6, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=2)\n",
    "    ])\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    conv_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    conv_history = conv_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return conv_model, conv_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "02b8e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_lstm_model(X_train, y_train, X_val, y_val, max_epochs=20, patience=2):\n",
    "    \"\"\"\n",
    "    Creates, compiles, and trains a Deep Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (array-like): Training input data\n",
    "    y_train (array-like): Training target data\n",
    "    X_val (array-like): Validation input data\n",
    "    y_val (array-like): Validation target data\n",
    "    max_epochs (int): Maximum number of training epochs\n",
    "    patience (int): Number of epochs with no improvement after which training will be stopped\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, history)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(X_train.shape) == 2:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    if len(X_val.shape) == 2:\n",
    "        X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "        \n",
    "    \n",
    "        \n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),  # Add Flatten layer to handle dimension mismatch\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError(), NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    lstm_history = lstm_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return lstm_model, lstm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e5f38b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 1914.3878 - mean_squared_error: 1914.3878 - NSE: -49.1645 - r_square: 0.1261 - val_loss: 6.5735 - val_mean_squared_error: 6.5735 - val_NSE: 0.8130 - val_r_square: 0.0175\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 13.1151 - mean_squared_error: 13.1151 - NSE: 0.6663 - r_square: 0.0750 - val_loss: 0.2962 - val_mean_squared_error: 0.2962 - val_NSE: 0.9916 - val_r_square: 0.0244\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 3.4577 - mean_squared_error: 3.4577 - NSE: 0.9113 - r_square: 0.1385 - val_loss: 0.1324 - val_mean_squared_error: 0.1324 - val_NSE: 0.9963 - val_r_square: 0.0224\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.9923 - mean_squared_error: 0.9923 - NSE: 0.9745 - r_square: 0.2439 - val_loss: 0.6002 - val_mean_squared_error: 0.6002 - val_NSE: 0.9829 - val_r_square: 0.0399\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.8150 - mean_squared_error: 0.8150 - NSE: 0.9789 - r_square: 0.4357 - val_loss: 0.8380 - val_mean_squared_error: 0.8380 - val_NSE: 0.9768 - val_r_square: 0.0539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "dnn_model, dnn_history = create_train_dnn_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6f83917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 15734.3877 - mean_squared_error: 15734.3877 - NSE: -408.9473 - r_square: 0.6374 - val_loss: 0.1656 - val_mean_squared_error: 0.1656 - val_NSE: 0.9954 - val_r_square: 0.1010\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 1.1951 - mean_squared_error: 1.1951 - NSE: 0.9697 - r_square: 0.5820 - val_loss: 0.2835 - val_mean_squared_error: 0.2835 - val_NSE: 0.9922 - val_r_square: 0.0863\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.3316 - mean_squared_error: 0.3316 - NSE: 0.9915 - r_square: 0.6645 - val_loss: 0.0616 - val_mean_squared_error: 0.0616 - val_NSE: 0.9983 - val_r_square: 0.0784\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.2429 - mean_squared_error: 0.2429 - NSE: 0.9937 - r_square: 0.7174 - val_loss: 0.0513 - val_mean_squared_error: 0.0513 - val_NSE: 0.9986 - val_r_square: 0.0806\n",
      "Epoch 5/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2159 - mean_squared_error: 0.2159 - NSE: 0.9944 - r_square: 0.7503 - val_loss: 0.1657 - val_mean_squared_error: 0.1657 - val_NSE: 0.9953 - val_r_square: 0.0799\n",
      "Epoch 6/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2197 - mean_squared_error: 0.2197 - NSE: 0.9943 - r_square: 0.7669 - val_loss: 0.0428 - val_mean_squared_error: 0.0428 - val_NSE: 0.9988 - val_r_square: 0.0808\n",
      "Epoch 7/20\n",
      "274/274 [==============================] - 1s 2ms/step - loss: 0.2332 - mean_squared_error: 0.2332 - NSE: 0.9940 - r_square: 0.7879 - val_loss: 0.9864 - val_mean_squared_error: 0.9864 - val_NSE: 0.9720 - val_r_square: 0.0816\n",
      "Epoch 8/20\n",
      "274/274 [==============================] - 0s 2ms/step - loss: 0.2533 - mean_squared_error: 0.2533 - NSE: 0.9934 - r_square: 0.8038 - val_loss: 0.1365 - val_mean_squared_error: 0.1365 - val_NSE: 0.9961 - val_r_square: 0.0816\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "cnn_model, cnn_history = create_train_cnn_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "128cc954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8760, 9)\n",
      "Y_train shape: (8760, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train_2016_2017.shape)\n",
    "print(\"Y_train shape:\", Y_train_2016_2017.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ce54da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "274/274 [==============================] - 3s 5ms/step - loss: 7.7471 - mean_squared_error: 7.7471 - NSE: 0.7984 - r_square: 0.0048 - val_loss: 0.1288 - val_mean_squared_error: 0.1288 - val_NSE: 0.9963 - val_r_square: 8.7892e-12\n",
      "Epoch 2/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2274 - mean_squared_error: 0.2274 - NSE: 0.9941 - r_square: 6.9606e-06 - val_loss: 0.1267 - val_mean_squared_error: 0.1267 - val_NSE: 0.9964 - val_r_square: 2.8278e-12\n",
      "Epoch 3/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2282 - mean_squared_error: 0.2282 - NSE: 0.9941 - r_square: 4.4230e-06 - val_loss: 0.1387 - val_mean_squared_error: 0.1387 - val_NSE: 0.9960 - val_r_square: 1.1879e-07\n",
      "Epoch 4/20\n",
      "274/274 [==============================] - 1s 3ms/step - loss: 0.2276 - mean_squared_error: 0.2276 - NSE: 0.9941 - r_square: 4.5478e-06 - val_loss: 0.1375 - val_mean_squared_error: 0.1375 - val_NSE: 0.9961 - val_r_square: 1.1839e-07\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "lstm_model, lstm_history = create_train_lstm_model(\n",
    "    X_train=X_train_2016_2017,\n",
    "    y_train=Y_train_2016_2017,\n",
    "    X_val=X_val_2016_2017,\n",
    "    y_val=Y_val_2016_2017,\n",
    "    max_epochs=20,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "151e30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 1.6420 - mean_squared_error: 1.6420 - NSE: 0.9594 - r_square: 0.1800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6420223712921143,\n",
       " 1.6420223712921143,\n",
       " 0.9594290256500244,\n",
       " 0.18002846837043762]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2016_2017 = np.array(test_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "\n",
    "dnn_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f6bd2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.1322 - mean_squared_error: 0.1322 - NSE: 0.9967 - r_square: 0.3573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13223299384117126,\n",
       " 0.13223299384117126,\n",
       " 0.9967476725578308,\n",
       " 0.3573175370693207]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2016_2017 = X_test_2016_2017.reshape(X_test_2016_2017.shape[0], X_test_2016_2017.shape[1], 1)\n",
    "\n",
    "cnn_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b8d4ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 1ms/step - loss: 0.1832 - mean_squared_error: 0.1832 - NSE: 0.9959 - r_square: 4.6070e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18316765129566193,\n",
       " 0.18316765129566193,\n",
       " 0.995904266834259,\n",
       " 4.607012670021504e-06]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_2016_2017 = np.array(test_2016_2017[['Rainfall_Aries', 'Rainfall_Boso', 'Rainfall_Campana', 'Rainfall_Nangka', 'Rainfall_Oro', 'x', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']].values.tolist())\n",
    "if len(X_test_2016_2017.shape) == 2:\n",
    "    X_test_2016_2017 = X_test_2016_2017.reshape((X_test_2016_2017.shape[0], 1, X_test_2016_2017.shape[1]))\n",
    "lstm_model.evaluate(X_test_2016_2017, Y_test_2016_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee341f83",
   "metadata": {},
   "source": [
    "## PINNs with window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2016_2017_pinn = train_2016_2017[['x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']]\n",
    "X_val_2016_2017_pinn = val_2016_2017[['x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']]\n",
    "X_test_2016_2017_pinn = test_2016_2017[['x','t', 'Discharge_Sto_Nino', 'friction_coeff', 'slope']]\n",
    "Y_train_2016_2017_pinn = train_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']]\n",
    "Y_val_2016_2017_pinn = val_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']]\n",
    "Y_test_2016_2017_pinn = test_2016_2017[['Velocity_Sto_Nino','Waterlevel_Sto_Nino']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, \n",
    "                 train_data, val_data, test_data):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = input_width + shift\n",
    "    \n",
    "    def make_dataset(self, data):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - self.total_window_size + 1):\n",
    "            inputs = data[i:i+self.input_width]\n",
    "            labels = data[i+self.input_width:i+self.total_window_size]\n",
    "            X.append(inputs)\n",
    "            Y.append(labels)\n",
    "        \n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_data)\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_data)\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_data)\n",
    "\n",
    "\n",
    "def create_pinn_model(input_shape, n1, n2, n3, reg):\n",
    "    def custom_loss_wrapper(model):\n",
    "        def loss(y_true, y_pred):\n",
    "            # Gradient computation and loss calculation\n",
    "            grads_u = K.gradients(model.output[:,:,0], model.input)[0]\n",
    "            grads_h = K.gradients(model.output[:,:,1], model.input)[0]\n",
    "            \n",
    "            du_dx = grads_u[:,:,0]\n",
    "            du_dt = grads_u[:,:,1]\n",
    "            dh_dx = grads_h[:,:,0]\n",
    "            \n",
    "            g = K.constant(9.8)\n",
    "            fric_coeff = model.input[:,:,3]\n",
    "            slope = model.input[:,:,4]\n",
    "            \n",
    "            # Saint-Venant equation loss\n",
    "            loss_saint_venant = du_dt + y_pred[:,:,0] * du_dx + g*dh_dx + g*slope + \\\n",
    "                                g*K.square(fric_coeff) * K.square(y_true[:,:,0]) / \\\n",
    "                                (K.pow(y_true[:,:,1], 4/3) + K.epsilon())\n",
    "            \n",
    "            l = K.mean(K.square(loss_saint_venant))\n",
    "            return 2*l + K.sum(K.mean(K.square(y_pred - y_true), axis=0))\n",
    "        return loss\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n1, activation='relu', kernel_regularizer=l2(reg), input_shape=input_shape),\n",
    "        Dense(n2, activation='relu', kernel_regularizer=l2(reg)),\n",
    "        Dense(n3, activation='relu', kernel_regularizer=l2(reg)),\n",
    "        Dense(2)\n",
    "    ])\n",
    "    \n",
    "    custom_loss = custom_loss_wrapper(model)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss=custom_loss, \n",
    "        metrics=['mape', 'mae', 'mse', NSE, r_square]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup and training\n",
    "\n",
    "'''\n",
    "The labels (Y variables) are typically handled separately during model training with model.fit(X_train, Y_train). \n",
    "The WindowGenerator is responsible for creating input sequences, while the corresponding labels are passed separately during model training.\n",
    "'''\n",
    "window = WindowGenerator(\n",
    "    input_width=6, \n",
    "    label_width=6, \n",
    "    shift=1, \n",
    "    train_data=X_train_2016_2017_pinn, \n",
    "    val_data=X_val_2016_2017_pinn, \n",
    "    test_data=X_test_2016_2017_pinn\n",
    ")\n",
    "\n",
    "X_train_2016_2017_pinn, Y_train_2016_2017_pinn = window.train\n",
    "X_val_2016_2017_pinn, Y_val_2016_2017_pinn = window.val\n",
    "X_test_2016_2017_pinn,Y_test_2016_2017_pinn = window.test\n",
    "\n",
    "# Create model with input shape from windowed data\n",
    "model = create_pinn_model(\n",
    "    input_shape=(6, 5),  # 6 time steps, 5 features\n",
    "    n1=64, \n",
    "    n2=32, \n",
    "    n3=16, \n",
    "    reg=0.001\n",
    ")\n",
    "\n",
    "# Training\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, verbose=False)\n",
    "history = model.fit(\n",
    "    X_train_2016_2017_pinn, Y_train_2016_2017_pinn, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val_2016_2017_pinn, Y_val_2016_2017_pinn), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "'''\n",
    "# Prediction\n",
    "def predict_future(model, initial_input, num_steps):\n",
    "    current_input = initial_input\n",
    "    predictions = []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        prediction = model.predict(current_input)\n",
    "        predictions.append(prediction[0, -1])\n",
    "        \n",
    "        current_input = np.roll(current_input, -1, axis=1)\n",
    "        current_input[0, -1] = prediction[0, -1]\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Predict from last test sequence\n",
    "last_input = X_test[-1:, :6, :]\n",
    "future_predictions = predict_future(model, last_input, num_steps=24)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1610957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
